{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d01fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "features = pd.read_csv('reg_features.csv')\n",
    "labels = pd.read_csv('labeled_bgl.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6eafbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 2) (559, 60)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape, features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37c3fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BGL  Label\n",
      "0   92.0      0\n",
      "1  301.0      2\n",
      "2  156.0      1\n",
      "3   94.0      0\n",
      "4   90.0      0\n",
      "Index(['BGL', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(labels.head())\n",
    "print(labels.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427b6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labeled_bgl.csv')['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47b1ae8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  Gender     HR  SPO2  maxBP  minBP  TGS2603_MEAN  TGS2603_IQR  \\\n",
      "0  26.0     0.0   93.0  98.0  120.0   80.0          2.73         0.09   \n",
      "1  64.0     0.0  101.0  98.0  140.0   80.0          2.74         0.09   \n",
      "2  79.0     0.0   71.0  99.0  150.0   70.0          2.74         0.41   \n",
      "3  65.0     0.0   70.0  97.0  130.0   80.0          2.98         0.05   \n",
      "4  45.0     0.0   56.0  99.0  110.0   70.0          2.41         0.95   \n",
      "\n",
      "   TGS2603_PTP  TGS2603_RMS  ...  TGS822_BW  MQ138_MEAN  MQ138_IQR  MQ138_PTP  \\\n",
      "0         0.13         2.73  ...       1.17        2.48       0.54       0.75   \n",
      "1         0.12         2.74  ...       1.04        2.41       0.11       0.17   \n",
      "2         1.49         2.77  ...       1.87        0.00       0.00       0.00   \n",
      "3         0.11         2.98  ...       0.10        2.36       0.15       0.20   \n",
      "4         1.69         2.48  ...       0.00        2.19       0.14       0.23   \n",
      "\n",
      "   MQ138_RMS  MQ138_INT  MQ138_SQ_INT  MQ138_ENERGY  MQ138_POWER  MQ138_BW  \n",
      "0       2.49       2.17          5.43         49.41       395.30      2.76  \n",
      "1       2.41       2.14          5.15         52.13       469.13      0.11  \n",
      "2       0.00       0.00          0.00          0.00         0.00      0.00  \n",
      "3       2.36       2.22          5.25         94.81      1611.76      0.08  \n",
      "4       2.19       2.13          4.67        177.58      6570.33      0.05  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "Index(['Age', 'Gender', 'HR', 'SPO2', 'maxBP', 'minBP', 'TGS2603_MEAN',\n",
      "       'TGS2603_IQR', 'TGS2603_PTP', 'TGS2603_RMS', 'TGS2603_INT',\n",
      "       'TGS2603_SQ_INT', 'TGS2603_ENERGY', 'TGS2603_POWER', 'TGS2603_BW',\n",
      "       'TGS2610_MEAN', 'TGS2610_RMS', 'TGS2610_INT', 'TGS2610_SQ_INT',\n",
      "       'TGS2610_ENERGY', 'TGS2610_POWER', 'TGS2610_CD', 'TGS2610_BW',\n",
      "       'TGS2602_MEAN', 'TGS2602_STD', 'TGS2602_IQR', 'TGS2602_PTP',\n",
      "       'TGS2602_RMS', 'TGS2602_INT', 'TGS2602_SQ_INT', 'TGS2602_ENERGY',\n",
      "       'TGS2602_POWER', 'TGS2602_BW', 'TGS2600_MEAN', 'TGS2600_IQR',\n",
      "       'TGS2600_PTP', 'TGS2600_RMS', 'TGS2600_INT', 'TGS2600_SQ_INT',\n",
      "       'TGS2600_ENERGY', 'TGS2600_POWER', 'TGS2600_BW', 'TGS822_MEAN',\n",
      "       'TGS822_IQR', 'TGS822_PTP', 'TGS822_RMS', 'TGS822_INT', 'TGS822_SQ_INT',\n",
      "       'TGS822_ENERGY', 'TGS822_POWER', 'TGS822_BW', 'MQ138_MEAN', 'MQ138_IQR',\n",
      "       'MQ138_PTP', 'MQ138_RMS', 'MQ138_INT', 'MQ138_SQ_INT', 'MQ138_ENERGY',\n",
      "       'MQ138_POWER', 'MQ138_BW'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(features.head())\n",
    "print(features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdef525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ab40a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513    1\n",
       "255    0\n",
       "313    0\n",
       "159    0\n",
       "514    2\n",
       "      ..\n",
       "537    0\n",
       "432    0\n",
       "136    0\n",
       "291    0\n",
       "515    2\n",
       "Name: Label, Length: 112, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7814cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            'C': np.logspace(-3, 3, 7),\n",
    "            'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7)),\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            'max_depth': [None, 10, 30, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 30],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boost\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 6]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training SVM =======\n",
      "Best Parameters: {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84        72\n",
      "           1       0.41      0.33      0.37        21\n",
      "           2       0.40      0.32      0.35        19\n",
      "\n",
      "    accuracy                           0.69       112\n",
      "   macro avg       0.54      0.51      0.52       112\n",
      "weighted avg       0.66      0.69      0.67       112\n",
      "\n",
      "\n",
      "======= Training Decision Tree =======\n",
      "Best Parameters: {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82        72\n",
      "           1       0.32      0.29      0.30        21\n",
      "           2       0.42      0.53      0.47        19\n",
      "\n",
      "    accuracy                           0.66       112\n",
      "   macro avg       0.52      0.54      0.53       112\n",
      "weighted avg       0.67      0.66      0.66       112\n",
      "\n",
      "\n",
      "======= Training Random Forest =======\n",
      "Best Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85        72\n",
      "           1       0.67      0.38      0.48        21\n",
      "           2       0.67      0.42      0.52        19\n",
      "\n",
      "    accuracy                           0.75       112\n",
      "   macro avg       0.70      0.58      0.62       112\n",
      "weighted avg       0.73      0.75      0.72       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89        72\n",
      "           1       0.54      0.33      0.41        21\n",
      "           2       0.43      0.53      0.48        19\n",
      "\n",
      "    accuracy                           0.74       112\n",
      "   macro avg       0.61      0.59      0.59       112\n",
      "weighted avg       0.73      0.74      0.73       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:51:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87        72\n",
      "           1       0.64      0.33      0.44        21\n",
      "           2       0.43      0.53      0.48        19\n",
      "\n",
      "    accuracy                           0.73       112\n",
      "   macro avg       0.63      0.59      0.59       112\n",
      "weighted avg       0.73      0.73      0.72       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.6875\n",
      "Precision: 0.6593\n",
      "Recall: 0.6875\n",
      "F1-Score: 0.6703\n",
      "Confusion Matrix:\n",
      "[[64  4  4]\n",
      " [ 9  7  5]\n",
      " [ 7  6  6]]\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6607\n",
      "Precision: 0.6703\n",
      "Recall: 0.6607\n",
      "F1-Score: 0.6640\n",
      "Confusion Matrix:\n",
      "[[58  7  7]\n",
      " [ 8  6  7]\n",
      " [ 3  6 10]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7348\n",
      "Recall: 0.7500\n",
      "F1-Score: 0.7249\n",
      "Confusion Matrix:\n",
      "[[68  2  2]\n",
      " [11  8  2]\n",
      " [ 9  2  8]]\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.7411\n",
      "Precision: 0.7330\n",
      "Recall: 0.7411\n",
      "F1-Score: 0.7313\n",
      "Confusion Matrix:\n",
      "[[66  0  6]\n",
      " [ 7  7  7]\n",
      " [ 3  6 10]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.7321\n",
      "Precision: 0.7288\n",
      "Recall: 0.7321\n",
      "F1-Score: 0.7200\n",
      "Confusion Matrix:\n",
      "[[65  1  6]\n",
      " [ 7  7  7]\n",
      " [ 6  3 10]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/evaluate models\n",
    "results = {}\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n======= Training {name} =======\")\n",
    "    \n",
    "    # Randomized search with 5-fold stratified CV\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config[\"model\"],\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=10,\n",
    "        cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare model performances\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995cbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e0b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0291a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11795cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e631e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data with stratification (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdb72a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425    0\n",
       "264    1\n",
       "481    0\n",
       "261    0\n",
       "187    0\n",
       "      ..\n",
       "5      0\n",
       "390    0\n",
       "431    0\n",
       "449    0\n",
       "128    2\n",
       "Name: Label, Length: 447, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33d79c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =pd.read_csv('bgl_train.csv')['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b065b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "922    1\n",
       "923    1\n",
       "924    0\n",
       "925    2\n",
       "926    1\n",
       "Name: Label, Length: 927, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17bfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdc55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70744fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training SVM =======\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87        72\n",
      "           1       0.53      0.48      0.50        21\n",
      "           2       0.67      0.63      0.65        19\n",
      "\n",
      "    accuracy                           0.77       112\n",
      "   macro avg       0.68      0.67      0.67       112\n",
      "weighted avg       0.76      0.77      0.76       112\n",
      "\n",
      "\n",
      "======= Training Decision Tree =======\n",
      "Best Parameters: {'splitter': 'random', 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78        72\n",
      "           1       0.38      0.29      0.32        21\n",
      "           2       0.38      0.32      0.34        19\n",
      "\n",
      "    accuracy                           0.63       112\n",
      "   macro avg       0.50      0.47      0.48       112\n",
      "weighted avg       0.61      0.63      0.62       112\n",
      "\n",
      "\n",
      "======= Training Random Forest =======\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 10, 'criterion': 'entropy'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87        72\n",
      "           1       0.56      0.48      0.51        21\n",
      "           2       0.62      0.53      0.57        19\n",
      "\n",
      "    accuracy                           0.76       112\n",
      "   macro avg       0.67      0.64      0.65       112\n",
      "weighted avg       0.75      0.76      0.75       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n",
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_depth': 2, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        72\n",
      "           1       0.42      0.38      0.40        21\n",
      "           2       0.57      0.63      0.60        19\n",
      "\n",
      "    accuracy                           0.71       112\n",
      "   macro avg       0.61      0.62      0.61       112\n",
      "weighted avg       0.71      0.71      0.71       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 1.0, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87        72\n",
      "           1       0.47      0.33      0.39        21\n",
      "           2       0.55      0.58      0.56        19\n",
      "\n",
      "    accuracy                           0.74       112\n",
      "   macro avg       0.62      0.61      0.61       112\n",
      "weighted avg       0.72      0.74      0.73       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.7679\n",
      "Precision: 0.7604\n",
      "Recall: 0.7679\n",
      "F1-Score: 0.7636\n",
      "Confusion Matrix:\n",
      "[[64  5  3]\n",
      " [ 8 10  3]\n",
      " [ 3  4 12]]\n",
      "balanced_accuracy 0.7807913860333215\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 100}\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6339\n",
      "Precision: 0.6080\n",
      "Recall: 0.6339\n",
      "F1-Score: 0.6180\n",
      "Confusion Matrix:\n",
      "[[59  7  6]\n",
      " [11  6  4]\n",
      " [10  3  6]]\n",
      "balanced_accuracy 0.6287915041947298\n",
      "Best Parameters: {'splitter': 'random', 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7589\n",
      "Precision: 0.7459\n",
      "Recall: 0.7589\n",
      "F1-Score: 0.7502\n",
      "Confusion Matrix:\n",
      "[[65  4  3]\n",
      " [ 8 10  3]\n",
      " [ 5  4 10]]\n",
      "balanced_accuracy 0.7612858915278271\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 10, 'criterion': 'entropy'}\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.7143\n",
      "Precision: 0.7116\n",
      "Recall: 0.7143\n",
      "F1-Score: 0.7125\n",
      "Confusion Matrix:\n",
      "[[60  7  5]\n",
      " [ 9  8  4]\n",
      " [ 3  4 12]]\n",
      "balanced_accuracy 0.7411733427862459\n",
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_depth': 2, 'learning_rate': 0.1}\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.7411\n",
      "Precision: 0.7235\n",
      "Recall: 0.7411\n",
      "F1-Score: 0.7295\n",
      "Confusion Matrix:\n",
      "[[65  3  4]\n",
      " [ 9  7  5]\n",
      " [ 3  5 11]]\n",
      "balanced_accuracy 0.7576568592697626\n",
      "Best Parameters: {'subsample': 1.0, 'reg_lambda': 1.0, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,balanced_accuracy_score \n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load features and labels\n",
    "features = pd.read_csv('reg_features_new.csv')\n",
    "labels = pd.read_csv('labeled_bgl.csv')['Label']\n",
    "\n",
    "\n",
    "\n",
    "# Split data with stratification (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "X_train.drop(columns=['Gender'], inplace=True)\n",
    "X_test\n",
    "\n",
    "\n",
    ".drop(columns=['Gender'], inplace=True)\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Initialize models and parameter grids\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            'C':      [0.1, 0.5, 1, 5, 10, 100],\n",
    "            'gamma':  ['scale', 'auto'],\n",
    "            'kernel': ['poly', 'rbf', 'sigmoid','linear']\n",
    "            # 'random_state': [10]\n",
    "            \n",
    "            }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random']\n",
    "            # 'random_state': [10]\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50,100, 200, 300],\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', 1.0]\n",
    "            # 'random_state': [10]\n",
    "            \n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boost\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth':[None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'min_samples_split': np.arange(2, 9)\n",
    "            # 'random_state': [10]  \n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': np.arange(50, 250, 50),\n",
    "            'learning_rate': [0.01, 0.1, 0.05, 0.5, 1.0],\n",
    "            'max_depth': np.arange(2, 11),\n",
    "            'subsample': [0.1, 0.5, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.1, 0.5, 0.8, 1.0],\n",
    "            \n",
    "            'reg_lambda': [0.1, 0.5, 0.8, 1.0],\n",
    "           \n",
    "            # 'random_state': [10],\n",
    "            'min_child_weight': [1, 2, 3, 4]\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Train/evaluate models\n",
    "results = {}\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n======= Training {name} =======\")\n",
    "    \n",
    "    # Randomized search with 5-fold stratified CV\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config[\"model\"],\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=10,\n",
    "        cv=StratifiedKFold(n_splits=5 , random_state=10, shuffle=True),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=10\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        # \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": (sensitivity_score(y_test, y_pred , average='weighted') + specificity_score(y_test, y_pred, average='weighted')) / 2,\n",
    "\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare model performances\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    # print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"balanced_accuracy\", metrics['balanced_accuracy'])\n",
    "    print(f\"Best Parameters: {metrics['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12eecff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training SVM =======\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82        72\n",
      "           1       0.71      0.24      0.36        21\n",
      "           2       0.71      0.26      0.38        19\n",
      "\n",
      "    accuracy                           0.71       112\n",
      "   macro avg       0.71      0.49      0.52       112\n",
      "weighted avg       0.71      0.71      0.66       112\n",
      "\n",
      "\n",
      "======= Training Decision Tree =======\n",
      "Best Parameters: {'splitter': 'best', 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_depth': 2, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.94      0.77        72\n",
      "           1       0.29      0.10      0.14        21\n",
      "           2       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.62       112\n",
      "   macro avg       0.31      0.35      0.31       112\n",
      "weighted avg       0.47      0.62      0.52       112\n",
      "\n",
      "\n",
      "======= Training Random Forest =======\n",
      "Best Parameters: {'n_estimators': 300, 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89        72\n",
      "           1       0.72      0.62      0.67        21\n",
      "           2       1.00      0.47      0.64        19\n",
      "\n",
      "    accuracy                           0.82       112\n",
      "   macro avg       0.85      0.69      0.73       112\n",
      "weighted avg       0.83      0.82      0.81       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 8, 'learning_rate': 0.01}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89        72\n",
      "           1       0.65      0.62      0.63        21\n",
      "           2       0.86      0.63      0.73        19\n",
      "\n",
      "    accuracy                           0.82       112\n",
      "   macro avg       0.79      0.73      0.75       112\n",
      "weighted avg       0.82      0.82      0.82       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n",
      "Best Parameters: {'subsample': 0.5, 'reg_lambda': 1.0, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89        72\n",
      "           1       0.68      0.62      0.65        21\n",
      "           2       0.86      0.63      0.73        19\n",
      "\n",
      "    accuracy                           0.82       112\n",
      "   macro avg       0.80      0.73      0.75       112\n",
      "weighted avg       0.82      0.82      0.82       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.7143\n",
      "Precision: 0.7143\n",
      "Recall: 0.7143\n",
      "F1-Score: 0.6616\n",
      "Confusion Matrix:\n",
      "[[70  0  2]\n",
      " [16  5  0]\n",
      " [12  2  5]]\n",
      "balanced_accuracy 0.628258300838946\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.4739\n",
      "Recall: 0.6250\n",
      "F1-Score: 0.5235\n",
      "Confusion Matrix:\n",
      "[[68  3  1]\n",
      " [19  2  0]\n",
      " [17  2  0]]\n",
      "balanced_accuracy 0.517151128441451\n",
      "Best Parameters: {'splitter': 'best', 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_depth': 2, 'criterion': 'gini'}\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.8214\n",
      "Precision: 0.8345\n",
      "Recall: 0.8214\n",
      "F1-Score: 0.8073\n",
      "Confusion Matrix:\n",
      "[[70  2  0]\n",
      " [ 8 13  0]\n",
      " [ 7  3  9]]\n",
      "balanced_accuracy 0.7850274725274724\n",
      "Best Parameters: {'n_estimators': 300, 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.8214\n",
      "Precision: 0.8195\n",
      "Recall: 0.8214\n",
      "F1-Score: 0.8166\n",
      "Confusion Matrix:\n",
      "[[67  4  1]\n",
      " [ 7 13  1]\n",
      " [ 4  3 12]]\n",
      "balanced_accuracy 0.8132857733664185\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 8, 'learning_rate': 0.01}\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.8214\n",
      "Precision: 0.8189\n",
      "Recall: 0.8214\n",
      "F1-Score: 0.8157\n",
      "Confusion Matrix:\n",
      "[[67  3  2]\n",
      " [ 8 13  0]\n",
      " [ 4  3 12]]\n",
      "balanced_accuracy 0.806280278860924\n",
      "Best Parameters: {'subsample': 0.5, 'reg_lambda': 1.0, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,balanced_accuracy_score \n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load features and labels\n",
    "# features = pd.read_csv('reg_features.csv')\n",
    "# labels = pd.read_csv('labeled_bgl.csv')['Label']\n",
    "\n",
    "\n",
    "# # Split data with stratification (80% train, 20% test)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "# )\n",
    "\n",
    "X_train = pd.read_csv('X_train_augmented1.csv')\n",
    "y_train = pd.read_csv('y_train_augmented.csv')['Label']\n",
    "X_test= pd.read_csv('X_test_original.csv')\n",
    "y_test = pd.read_csv('y_test_original.csv')['Label']\n",
    "\n",
    "# X_train.drop(columns=['Gender'], inplace=True)\n",
    "# X_test.drop(columns=['Gender'], inplace=True)\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models and parameter grids\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            'C':      [0.1, 0.5, 1, 5, 10, 100],\n",
    "            'gamma':  ['scale', 'auto'],\n",
    "            'kernel': ['poly', 'rbf', 'sigmoid','linear']\n",
    "            \n",
    "            \n",
    "            }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random']\n",
    "\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50,100, 200, 300],\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', 1.0]\n",
    "            \n",
    "            \n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boost\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth':[ None,2,3, 5, 7, 8, 10],\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'min_samples_split': np.arange(2, 9)\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': np.arange(50, 250, 50),\n",
    "            'learning_rate': [0.01, 0.1, 0.05, 0.5, 1.0],\n",
    "            'max_depth': np.arange(2, 11),\n",
    "            'subsample': [0.1, 0.5, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.1, 0.5, 0.8, 1.0],\n",
    "            \n",
    "            'reg_lambda': [0.1, 0.5, 0.8, 1.0],\n",
    "            'min_child_weight': [1, 2, 3, 4]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train/evaluate models\n",
    "results = {}\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n======= Training {name} =======\")\n",
    "    \n",
    "    # Randomized search with 5-fold stratified CV\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config[\"model\"],\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=20,\n",
    "        cv=StratifiedKFold(n_splits=5 , random_state=10, shuffle=True),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=10\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        # \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": (sensitivity_score(y_test, y_pred , average='weighted') + specificity_score(y_test, y_pred, average='weighted')) / 2,\n",
    "\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare model performances\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    # print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"balanced_accuracy\", metrics['balanced_accuracy'])\n",
    "    print(f\"Best Parameters: {metrics['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20af860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training SVM =======\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85        72\n",
      "           1       0.54      0.33      0.41        21\n",
      "           2       0.88      0.37      0.52        19\n",
      "\n",
      "    accuracy                           0.74       112\n",
      "   macro avg       0.72      0.55      0.59       112\n",
      "weighted avg       0.74      0.74      0.71       112\n",
      "\n",
      "\n",
      "======= Training Decision Tree =======\n",
      "Best Parameters: {'splitter': 'random', 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84        72\n",
      "           1       0.54      0.67      0.60        21\n",
      "           2       0.88      0.37      0.52        19\n",
      "\n",
      "    accuracy                           0.75       112\n",
      "   macro avg       0.74      0.64      0.65       112\n",
      "weighted avg       0.77      0.75      0.74       112\n",
      "\n",
      "\n",
      "======= Training Random Forest =======\n",
      "Best Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88        72\n",
      "           1       0.75      0.57      0.65        21\n",
      "           2       1.00      0.58      0.73        19\n",
      "\n",
      "    accuracy                           0.82       112\n",
      "   macro avg       0.85      0.70      0.75       112\n",
      "weighted avg       0.83      0.82      0.81       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90        72\n",
      "           1       0.68      0.62      0.65        21\n",
      "           2       0.92      0.58      0.71        19\n",
      "\n",
      "    accuracy                           0.83       112\n",
      "   macro avg       0.82      0.72      0.75       112\n",
      "weighted avg       0.83      0.83      0.82       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n",
      "Best Parameters: {'subsample': 0.5, 'reg_lambda': 1.0, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90        72\n",
      "           1       0.76      0.62      0.68        21\n",
      "           2       0.85      0.58      0.69        19\n",
      "\n",
      "    accuracy                           0.83       112\n",
      "   macro avg       0.82      0.72      0.76       112\n",
      "weighted avg       0.83      0.83      0.82       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.7411\n",
      "Precision: 0.7368\n",
      "Recall: 0.7411\n",
      "F1-Score: 0.7094\n",
      "Confusion Matrix:\n",
      "[[69  2  1]\n",
      " [14  7  0]\n",
      " [ 8  4  7]]\n",
      "balanced_accuracy 0.6866566229469455\n",
      "Best Parameters: {'kernel': 'poly', 'gamma': 'scale', 'C': 100}\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7686\n",
      "Recall: 0.7500\n",
      "F1-Score: 0.7397\n",
      "Confusion Matrix:\n",
      "[[63  8  1]\n",
      " [ 7 14  0]\n",
      " [ 8  4  7]]\n",
      "balanced_accuracy 0.7411895899799126\n",
      "Best Parameters: {'splitter': 'random', 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.8214\n",
      "Precision: 0.8321\n",
      "Recall: 0.8214\n",
      "F1-Score: 0.8111\n",
      "Confusion Matrix:\n",
      "[[69  3  0]\n",
      " [ 9 12  0]\n",
      " [ 7  1 11]]\n",
      "balanced_accuracy 0.7780219780219779\n",
      "Best Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.8304\n",
      "Precision: 0.8314\n",
      "Recall: 0.8304\n",
      "F1-Score: 0.8221\n",
      "Confusion Matrix:\n",
      "[[69  2  1]\n",
      " [ 8 13  0]\n",
      " [ 4  4 11]]\n",
      "balanced_accuracy 0.8116566229469455\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.8304\n",
      "Precision: 0.8279\n",
      "Recall: 0.8304\n",
      "F1-Score: 0.8210\n",
      "Confusion Matrix:\n",
      "[[69  2  1]\n",
      " [ 7 13  1]\n",
      " [ 6  2 11]]\n",
      "balanced_accuracy 0.804769289849935\n",
      "Best Parameters: {'subsample': 0.5, 'reg_lambda': 1.0, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,balanced_accuracy_score \n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load features and labels\n",
    "# features = pd.read_csv('reg_features.csv')\n",
    "# labels = pd.read_csv('labeled_bgl.csv')['Label']\n",
    "\n",
    "\n",
    "# # Split data with stratification (80% train, 20% test)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "# )\n",
    "\n",
    "X_train = pd.read_csv('X_train_augmented.csv')\n",
    "y_train = pd.read_csv('y_train_augmented.csv')['Label']\n",
    "X_test= pd.read_csv('X_test_original.csv')\n",
    "y_test = pd.read_csv('y_test_original.csv')['Label']\n",
    "# X_train.drop(columns=['Gender'], inplace=True)\n",
    "# X_test.drop(columns=['Gender'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models and parameter grids\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            'C':      [0.1, 0.5, 1, 5, 10, 100],\n",
    "            'gamma':  ['scale', 'auto'],\n",
    "            'kernel': ['poly', 'rbf', 'sigmoid','linear']\n",
    "            \n",
    "            \n",
    "            }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random']\n",
    "\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50,100, 200, 300],\n",
    "            'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "            'min_samples_split': np.arange(2, 9),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', 1.0]\n",
    "            \n",
    "            \n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boost\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth':[ 3, 5, 7, 8, 10],\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'min_samples_split': np.arange(2, 9)\n",
    "\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            'n_estimators': np.arange(50, 250, 50),\n",
    "            'learning_rate': [0.01, 0.1, 0.05, 0.5, 1.0],\n",
    "            'max_depth': np.arange(2, 11),\n",
    "            'subsample': [0.1, 0.5, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.1, 0.5, 0.8, 1.0],\n",
    "            \n",
    "            'reg_lambda': [0.1, 0.5, 0.8, 1.0],\n",
    "            'min_child_weight': [1, 2, 3, 4]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train/evaluate models\n",
    "results = {}\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n======= Training {name} =======\")\n",
    "    \n",
    "    # Randomized search with 5-fold stratified CV\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config[\"model\"],\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=10,\n",
    "        cv=StratifiedKFold(n_splits=5 , random_state=10, shuffle=True),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=10\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        # \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": (sensitivity_score(y_test, y_pred , average='weighted') + specificity_score(y_test, y_pred, average='weighted')) / 2,\n",
    "\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best Parameters: {search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare model performances\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    # print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"balanced_accuracy\", metrics['balanced_accuracy'])\n",
    "    print(f\"Best Parameters: {metrics['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f53ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481eeffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e847e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f4709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b57131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529caaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7845f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training SVM =======\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for SVM: {'kernel': 'rbf', 'gamma': 0.1, 'C': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84        72\n",
      "           1       0.55      0.29      0.38        21\n",
      "           2       0.71      0.26      0.38        19\n",
      "\n",
      "    accuracy                           0.72       112\n",
      "   macro avg       0.67      0.51      0.53       112\n",
      "weighted avg       0.70      0.72      0.68       112\n",
      "\n",
      "\n",
      "======= Training Decision Tree =======\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for Decision Tree: {'splitter': 'best', 'min_samples_split': 11, 'min_samples_leaf': 10, 'max_depth': 5, 'criterion': 'entropy'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.77        72\n",
      "           1       0.36      0.24      0.29        21\n",
      "           2       0.38      0.16      0.22        19\n",
      "\n",
      "    accuracy                           0.62       112\n",
      "   macro avg       0.47      0.42      0.42       112\n",
      "weighted avg       0.57      0.62      0.58       112\n",
      "\n",
      "\n",
      "======= Training Random Forest =======\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 7, 'max_features': 'sqrt', 'max_depth': 15, 'criterion': 'entropy'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.90        72\n",
      "           1       0.76      0.62      0.68        21\n",
      "           2       1.00      0.53      0.69        19\n",
      "\n",
      "    accuracy                           0.84       112\n",
      "   macro avg       0.87      0.71      0.76       112\n",
      "weighted avg       0.85      0.84      0.83       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for Gradient Boost: {'subsample': 1.0, 'n_estimators': 300, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_depth': 10, 'learning_rate': 0.2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92        72\n",
      "           1       0.68      0.62      0.65        21\n",
      "           2       0.92      0.58      0.71        19\n",
      "\n",
      "    accuracy                           0.84       112\n",
      "   macro avg       0.82      0.72      0.76       112\n",
      "weighted avg       0.84      0.84      0.83       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\suryansh\\anaconda3\\envs\\cuda\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:21:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'subsample': 0.7, 'reg_lambda': 0.5, 'reg_alpha': 0.5, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 9, 'learning_rate': 0.2, 'colsample_bytree': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89        72\n",
      "           1       0.76      0.62      0.68        21\n",
      "           2       0.80      0.63      0.71        19\n",
      "\n",
      "    accuracy                           0.83       112\n",
      "   macro avg       0.80      0.73      0.76       112\n",
      "weighted avg       0.83      0.83      0.82       112\n",
      "\n",
      "\n",
      "======= Training Voting Classifier =======\n",
      "\n",
      "===== Voting Classifier Performance (Hard Voting) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91        72\n",
      "           1       0.76      0.62      0.68        21\n",
      "           2       1.00      0.58      0.73        19\n",
      "\n",
      "    accuracy                           0.85       112\n",
      "   macro avg       0.87      0.73      0.78       112\n",
      "weighted avg       0.86      0.85      0.84       112\n",
      "\n",
      "\n",
      "===== Voting Classifier Performance (Soft Voting) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92        72\n",
      "           1       0.76      0.62      0.68        21\n",
      "           2       0.92      0.58      0.71        19\n",
      "\n",
      "    accuracy                           0.85       112\n",
      "   macro avg       0.85      0.73      0.77       112\n",
      "weighted avg       0.85      0.85      0.84       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "                accuracy  precision    recall        f1  balanced_accuracy\n",
      "Gradient Boost  0.839286   0.839351  0.839286  0.830502           0.824157\n",
      "Voting_Soft     0.848214   0.848802  0.848214  0.837621           0.822646\n",
      "Voting_Hard     0.848214   0.856393  0.848214  0.837859           0.815522\n",
      "XGBoost         0.830357   0.825525  0.830357  0.823225           0.811893\n",
      "Random Forest   0.839286   0.850000  0.839286  0.826722           0.803022\n",
      "SVM             0.723214   0.702170  0.723214  0.677728           0.661775\n",
      "Decision Tree   0.625000   0.573437  0.625000  0.583333           0.573668\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.7232\n",
      "Precision: 0.7022\n",
      "Recall: 0.7232\n",
      "F1-Score: 0.6777\n",
      "Balanced Accuracy: 0.6618\n",
      "Confusion Matrix:\n",
      "[[70  0  2]\n",
      " [15  6  0]\n",
      " [ 9  5  5]]\n",
      "Best Parameters: {'kernel': 'rbf', 'gamma': 0.1, 'C': 100}\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.5734\n",
      "Recall: 0.6250\n",
      "F1-Score: 0.5833\n",
      "Balanced Accuracy: 0.5737\n",
      "Confusion Matrix:\n",
      "[[62  7  3]\n",
      " [14  5  2]\n",
      " [14  2  3]]\n",
      "Best Parameters: {'splitter': 'best', 'min_samples_split': 11, 'min_samples_leaf': 10, 'max_depth': 5, 'criterion': 'entropy'}\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.8393\n",
      "Precision: 0.8500\n",
      "Recall: 0.8393\n",
      "F1-Score: 0.8267\n",
      "Balanced Accuracy: 0.8030\n",
      "Confusion Matrix:\n",
      "[[71  1  0]\n",
      " [ 8 13  0]\n",
      " [ 6  3 10]]\n",
      "Best Parameters: {'n_estimators': 300, 'min_samples_split': 7, 'max_features': 'sqrt', 'max_depth': 15, 'criterion': 'entropy'}\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.8393\n",
      "Precision: 0.8394\n",
      "Recall: 0.8393\n",
      "F1-Score: 0.8305\n",
      "Balanced Accuracy: 0.8242\n",
      "Confusion Matrix:\n",
      "[[70  2  0]\n",
      " [ 7 13  1]\n",
      " [ 4  4 11]]\n",
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 300, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_depth': 10, 'learning_rate': 0.2}\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.8304\n",
      "Precision: 0.8255\n",
      "Recall: 0.8304\n",
      "F1-Score: 0.8232\n",
      "Balanced Accuracy: 0.8119\n",
      "Confusion Matrix:\n",
      "[[68  2  2]\n",
      " [ 7 13  1]\n",
      " [ 5  2 12]]\n",
      "Best Parameters: {'subsample': 0.7, 'reg_lambda': 0.5, 'reg_alpha': 0.5, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 9, 'learning_rate': 0.2, 'colsample_bytree': 1.0}\n",
      "\n",
      "Voting_Hard:\n",
      "Accuracy: 0.8482\n",
      "Precision: 0.8564\n",
      "Recall: 0.8482\n",
      "F1-Score: 0.8379\n",
      "Balanced Accuracy: 0.8155\n",
      "Confusion Matrix:\n",
      "[[71  1  0]\n",
      " [ 8 13  0]\n",
      " [ 5  3 11]]\n",
      "\n",
      "Voting_Soft:\n",
      "Accuracy: 0.8482\n",
      "Precision: 0.8488\n",
      "Recall: 0.8482\n",
      "F1-Score: 0.8376\n",
      "Balanced Accuracy: 0.8226\n",
      "Confusion Matrix:\n",
      "[[71  1  0]\n",
      " [ 7 13  1]\n",
      " [ 5  3 11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,balanced_accuracy_score\n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load features and labels\n",
    "X_train = pd.read_csv('X_train_augmented1.csv')\n",
    "y_train = pd.read_csv('y_train_augmented1.csv')['Label']\n",
    "X_test= pd.read_csv('X_test_original.csv')\n",
    "y_test = pd.read_csv('y_test_original.csv')['Label']\n",
    "\n",
    "# Drop 'Gender' if it's an issue and not handled as a categorical feature\n",
    "# If 'Gender' is categorical and needs encoding, do it before scaling.\n",
    "# For simplicity in this example, let's assume it's already handled or dropped.\n",
    "# X_train.drop(columns=['Gender'], inplace=True, errors='ignore')\n",
    "# X_test.drop(columns=['Gender'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames with original column names\n",
    "# This is useful if you intend to pass DataFrames to models (though most sklearn accept arrays)\n",
    "# And especially if you want to inspect features later.\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "\n",
    "# Initialize models and parameter grids for individual models\n",
    "# It's crucial to find good individual models before combining them.\n",
    "# I've slightly adjusted some parameter ranges for more exploration.\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True, random_state=10), # probability=True is needed for 'soft' voting\n",
    "        \"params\": {\n",
    "            'C':        [0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "            'gamma':    ['scale', 'auto', 0.01, 0.1, 0.5],\n",
    "            'kernel':   ['rbf', 'poly', 'sigmoid','linear'] # rbf is generally a good default\n",
    "        }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(random_state=10),\n",
    "        \"params\": {\n",
    "            'max_depth': [None, 3, 5, 7, 10, 15, 20],\n",
    "            'min_samples_split': np.arange(2, 15),\n",
    "            'min_samples_leaf': [1, 2, 3, 4, 5, 10],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=10),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300, 500],\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': np.arange(2, 10),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', 0.8, 1.0] # 0.8 is a proportion\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boost\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=10),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300, 500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth':[3, 5, 7, 10], # Typically smaller for boosting\n",
    "            'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'min_samples_split': np.arange(2, 8)\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=10), # Suppress warning and set eval_metric\n",
    "        \"params\": {\n",
    "            'n_estimators': np.arange(100, 500, 100),\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth': np.arange(3, 10),\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_lambda': [0.1, 0.5, 1.0, 2.0],\n",
    "            'reg_alpha': [0.0, 0.1, 0.5, 1.0], # L1 regularization\n",
    "            'min_child_weight': [1, 2, 3, 4]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Step 1: Tune Individual Models and Store Best Estimators ---\n",
    "best_estimators = {}\n",
    "results = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n======= Training {name} =======\")\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config[\"model\"],\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=20, # Increased n_iter for more thorough search\n",
    "        cv=StratifiedKFold(n_splits=5 , random_state=10, shuffle=True),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=10,\n",
    "        verbose=1 # Added verbose to see progress\n",
    "    )\n",
    "    search.fit(X_train_scaled_df, y_train) # Using df here, but array is fine\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "    best_estimators[name] = best_model # Store the best individual model\n",
    "\n",
    "    y_pred = best_model.predict(X_test_scaled_df)\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        \"balanced_accuracy\": (sensitivity_score(y_test, y_pred , average='weighted') + specificity_score(y_test, y_pred, average='weighted')) / 2,\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "\n",
    "    print(f\"Best Parameters for {name}: {search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Step 2: Implement Voting Classifier ---\n",
    "print(\"\\n======= Training Voting Classifier =======\")\n",
    "\n",
    "# Select the best performing individual models for the ensemble\n",
    "# You might choose all, or a subset based on individual performance and diversity.\n",
    "# For 'soft' voting, models must have 'predict_proba' method (SVM needs probability=True).\n",
    "# You can adjust weights if some models are more trustworthy than others.\n",
    "ensemble_estimators = [\n",
    "    ('svm', best_estimators['SVM']),\n",
    "    ('dt', best_estimators['Decision Tree']),\n",
    "    ('rf', best_estimators['Random Forest']),\n",
    "    ('gb', best_estimators['Gradient Boost']),\n",
    "    ('xgb', best_estimators['XGBoost'])\n",
    "]\n",
    "\n",
    "# Hard Voting (majority class vote)\n",
    "voting_clf_hard = VotingClassifier(estimators=ensemble_estimators, voting='hard', n_jobs=-1)\n",
    "voting_clf_hard.fit(X_train_scaled_df, y_train)\n",
    "y_pred_hard_voting = voting_clf_hard.predict(X_test_scaled_df)\n",
    "\n",
    "# Soft Voting (average of predicted probabilities, requires predict_proba)\n",
    "# This often performs better as it considers confidence.\n",
    "# Make sure SVM has probability=True when initialized.\n",
    "voting_clf_soft = VotingClassifier(estimators=ensemble_estimators, voting='soft', n_jobs=-1)\n",
    "voting_clf_soft.fit(X_train_scaled_df, y_train)\n",
    "y_pred_soft_voting = voting_clf_soft.predict(X_test_scaled_df)\n",
    "\n",
    "\n",
    "# --- Step 3: Evaluate Voting Classifier ---\n",
    "print(\"\\n===== Voting Classifier Performance (Hard Voting) =====\")\n",
    "results['Voting_Hard'] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_hard_voting),\n",
    "    \"precision\": precision_score(y_test, y_pred_hard_voting, average='weighted'),\n",
    "    \"recall\": recall_score(y_test, y_pred_hard_voting, average='weighted'),\n",
    "    \"f1\": f1_score(y_test, y_pred_hard_voting, average='weighted'),\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, y_pred_hard_voting),\n",
    "    \"balanced_accuracy\": (sensitivity_score(y_test, y_pred_hard_voting , average='weighted') + specificity_score(y_test, y_pred_hard_voting, average='weighted')) / 2,\n",
    "    \"best_params\": \"N/A (Ensemble)\"\n",
    "}\n",
    "print(classification_report(y_test, y_pred_hard_voting))\n",
    "\n",
    "\n",
    "print(\"\\n===== Voting Classifier Performance (Soft Voting) =====\")\n",
    "results['Voting_Soft'] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_soft_voting),\n",
    "    \"precision\": precision_score(y_test, y_pred_soft_voting, average='weighted'),\n",
    "    \"recall\": recall_score(y_test, y_pred_soft_voting, average='weighted'),\n",
    "    \"f1\": f1_score(y_test, y_pred_soft_voting, average='weighted'),\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, y_pred_soft_voting),\n",
    "    \"balanced_accuracy\": (sensitivity_score(y_test, y_pred_soft_voting , average='weighted') + specificity_score(y_test, y_pred_soft_voting, average='weighted')) / 2,\n",
    "    \"best_params\": \"N/A (Ensemble)\"\n",
    "}\n",
    "print(classification_report(y_test, y_pred_soft_voting))\n",
    "\n",
    "# --- Step 4: Compare All Model Performances ---\n",
    "print(\"\\n===== Model Performance Summary =====\")\n",
    "# Convert to a DataFrame for easier comparison and sorting\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "results_df['balanced_accuracy'] = results_df['balanced_accuracy'].astype(float) # Ensure it's numeric for sorting\n",
    "print(results_df[['accuracy', 'precision', 'recall', 'f1', 'balanced_accuracy']].sort_values(by='balanced_accuracy', ascending=False))\n",
    "\n",
    "\n",
    "# Print detailed results for all models including ensembles\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "    if model not in ['Voting_Hard', 'Voting_Soft']: # Best params only for individual models\n",
    "        print(f\"Best Parameters: {metrics['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50607a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['Gender'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a57afc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>HR</th>\n",
       "      <th>SPO2</th>\n",
       "      <th>maxBP</th>\n",
       "      <th>minBP</th>\n",
       "      <th>TGS2603_MEAN</th>\n",
       "      <th>TGS2603_IQR</th>\n",
       "      <th>TGS2603_PTP</th>\n",
       "      <th>TGS2603_RMS</th>\n",
       "      <th>TGS2603_INT</th>\n",
       "      <th>...</th>\n",
       "      <th>TGS822_BW</th>\n",
       "      <th>MQ138_MEAN</th>\n",
       "      <th>MQ138_IQR</th>\n",
       "      <th>MQ138_PTP</th>\n",
       "      <th>MQ138_RMS</th>\n",
       "      <th>MQ138_INT</th>\n",
       "      <th>MQ138_SQ_INT</th>\n",
       "      <th>MQ138_ENERGY</th>\n",
       "      <th>MQ138_POWER</th>\n",
       "      <th>MQ138_BW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.00</td>\n",
       "      <td>86.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>126.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.57</td>\n",
       "      <td>...</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.47</td>\n",
       "      <td>6.33</td>\n",
       "      <td>270.73</td>\n",
       "      <td>11370.55</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7.23</td>\n",
       "      <td>101.87</td>\n",
       "      <td>1324.31</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>17.01</td>\n",
       "      <td>52.55</td>\n",
       "      <td>97.98</td>\n",
       "      <td>82.23</td>\n",
       "      <td>85.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.66</td>\n",
       "      <td>21.30</td>\n",
       "      <td>1718.70</td>\n",
       "      <td>135848.19</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>17.02</td>\n",
       "      <td>51.35</td>\n",
       "      <td>98.76</td>\n",
       "      <td>81.43</td>\n",
       "      <td>102.66</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2.61</td>\n",
       "      <td>...</td>\n",
       "      <td>23.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.00</td>\n",
       "      <td>14.19</td>\n",
       "      <td>1680.73</td>\n",
       "      <td>23753.07</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>73.95</td>\n",
       "      <td>66.56</td>\n",
       "      <td>91.87</td>\n",
       "      <td>202.15</td>\n",
       "      <td>114.79</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.59</td>\n",
       "      <td>21.81</td>\n",
       "      <td>659.18</td>\n",
       "      <td>2314.41</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>29.46</td>\n",
       "      <td>51.30</td>\n",
       "      <td>99.00</td>\n",
       "      <td>82.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.59</td>\n",
       "      <td>...</td>\n",
       "      <td>31.49</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.68</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.27</td>\n",
       "      <td>634.42</td>\n",
       "      <td>11091.71</td>\n",
       "      <td>5.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>17.00</td>\n",
       "      <td>52.39</td>\n",
       "      <td>99.00</td>\n",
       "      <td>81.02</td>\n",
       "      <td>58.61</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>32.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1343.93</td>\n",
       "      <td>210815.16</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1947 rows  59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age     HR   SPO2   maxBP   minBP  TGS2603_MEAN  TGS2603_IQR  \\\n",
       "0     40.00  86.00  98.00  126.00   84.00          1.48         0.18   \n",
       "1     58.00  90.00  98.00  137.00   87.00          1.74         0.25   \n",
       "2     22.00  68.00  96.00  116.00   68.00          2.63         0.65   \n",
       "3     68.00  90.00  96.00  116.00   74.00          1.88         0.45   \n",
       "4     22.00  68.00  96.00  116.00   68.00          2.78         0.09   \n",
       "...     ...    ...    ...     ...     ...           ...          ...   \n",
       "1942  17.01  52.55  97.98   82.23   85.50          2.17         1.25   \n",
       "1943  17.02  51.35  98.76   81.43  102.66          2.93         0.94   \n",
       "1944  73.95  66.56  91.87  202.15  114.79          3.15         0.00   \n",
       "1945  29.46  51.30  99.00   82.00  115.00          3.15         0.00   \n",
       "1946  17.00  52.39  99.00   81.02   58.61          0.18         1.33   \n",
       "\n",
       "      TGS2603_PTP  TGS2603_RMS  TGS2603_INT  ...  TGS822_BW  MQ138_MEAN  \\\n",
       "0            0.27         1.48         1.45  ...       0.02        0.00   \n",
       "1            0.41         1.74         1.68  ...       0.55        0.00   \n",
       "2            1.58         2.68         2.57  ...       2.62        2.53   \n",
       "3            0.72         1.90         1.82  ...       0.03        0.00   \n",
       "4            0.12         2.78         2.50  ...       0.64        2.80   \n",
       "...           ...          ...          ...  ...        ...         ...   \n",
       "1942         1.82         0.27         1.36  ...       1.20        0.01   \n",
       "1943         0.35         0.09         2.61  ...      23.34        0.01   \n",
       "1944         0.28         0.00         2.65  ...       0.15        0.30   \n",
       "1945         0.00         0.06         1.59  ...      31.49        0.29   \n",
       "1946         0.43         2.92         0.59  ...      32.42        0.02   \n",
       "\n",
       "      MQ138_IQR  MQ138_PTP  MQ138_RMS  MQ138_INT  MQ138_SQ_INT  MQ138_ENERGY  \\\n",
       "0          0.00       0.00       0.00       0.00          0.00          0.00   \n",
       "1          0.00       0.00       0.00       0.00          0.00          0.00   \n",
       "2          0.58       0.80       2.55       2.47          6.33        270.73   \n",
       "3          0.00       0.00       0.00       0.00          0.00          0.00   \n",
       "4          0.04       0.06       2.80       2.58          7.23        101.87   \n",
       "...         ...        ...        ...        ...           ...           ...   \n",
       "1942       0.00       1.56       0.06       4.66         21.30       1718.70   \n",
       "1943       0.02       1.55       1.49       4.00         14.19       1680.73   \n",
       "1944       0.27       0.66       4.46       1.59         21.81        659.18   \n",
       "1945       0.03       0.00       4.68       2.58          0.27        634.42   \n",
       "1946       0.12       1.56       0.13       0.39          3.55       1343.93   \n",
       "\n",
       "      MQ138_POWER  MQ138_BW  \n",
       "0            0.00      0.00  \n",
       "1            0.00      0.00  \n",
       "2        11370.55      0.44  \n",
       "3            0.00      0.00  \n",
       "4         1324.31      0.01  \n",
       "...           ...       ...  \n",
       "1942    135848.19      0.03  \n",
       "1943     23753.07      0.08  \n",
       "1944      2314.41      0.06  \n",
       "1945     11091.71      5.47  \n",
       "1946    210815.16      0.61  \n",
       "\n",
       "[1947 rows x 59 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79dac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Training Random Forest =======\n",
      "Best Parameters: {'random_state': 10, 'n_estimators': 300, 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85        72\n",
      "           1       0.64      0.43      0.51        21\n",
      "           2       0.67      0.53      0.59        19\n",
      "\n",
      "    accuracy                           0.76       112\n",
      "   macro avg       0.70      0.62      0.65       112\n",
      "weighted avg       0.74      0.76      0.74       112\n",
      "\n",
      "\n",
      "======= Training Gradient Boost =======\n",
      "Best Parameters: {'subsample': 0.8, 'random_state': 10, 'n_estimators': 200, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        72\n",
      "           1       0.53      0.38      0.44        21\n",
      "           2       0.55      0.58      0.56        19\n",
      "\n",
      "    accuracy                           0.76       112\n",
      "   macro avg       0.65      0.63      0.63       112\n",
      "weighted avg       0.74      0.76      0.75       112\n",
      "\n",
      "\n",
      "======= Training XGBoost =======\n",
      "Best Parameters: {'subsample': 1.0, 'random_state': 10, 'n_estimators': 100, 'max_depth': None, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86        72\n",
      "           1       0.47      0.33      0.39        21\n",
      "           2       0.48      0.53      0.50        19\n",
      "\n",
      "    accuracy                           0.72       112\n",
      "   macro avg       0.59      0.58      0.58       112\n",
      "weighted avg       0.71      0.72      0.71       112\n",
      "\n",
      "\n",
      "===== Model Performance Summary =====\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7589\n",
      "Precision: 0.7448\n",
      "Recall: 0.7589\n",
      "F1-Score: 0.7437\n",
      "Confusion Matrix:\n",
      "[[66  3  3]\n",
      " [10  9  2]\n",
      " [ 7  2 10]]\n",
      "balanced_accuracy 0.733145752097365\n",
      "Best Parameters: {'random_state': 10, 'n_estimators': 300, 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "\n",
      "Gradient Boost:\n",
      "Accuracy: 0.7589\n",
      "Precision: 0.7443\n",
      "Recall: 0.7589\n",
      "F1-Score: 0.7485\n",
      "Confusion Matrix:\n",
      "[[66  2  4]\n",
      " [ 8  8  5]\n",
      " [ 3  5 11]]\n",
      "balanced_accuracy 0.775651364764268\n",
      "Best Parameters: {'subsample': 0.8, 'random_state': 10, 'n_estimators': 200, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.7232\n",
      "Precision: 0.7096\n",
      "Recall: 0.7232\n",
      "F1-Score: 0.7137\n",
      "Confusion Matrix:\n",
      "[[64  3  5]\n",
      " [ 8  7  6]\n",
      " [ 4  5 10]]\n",
      "balanced_accuracy 0.7469041710977194\n",
      "Best Parameters: {'subsample': 1.0, 'random_state': 10, 'n_estimators': 100, 'max_depth': None, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,balanced_accuracy_score \n",
    "# from imblearn.metrics import specificity_score, sensitivity_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "        \n",
    "# # # Load features and labels\n",
    "# # features = pd.read_csv('reg_features.csv')\n",
    "# # labels = pd.read_csv('labeled_bgl.csv')['Label']\n",
    "\n",
    "\n",
    "# # # Split data with stratification (80% train, 20% test)\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(\n",
    "# #     features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "# # )\n",
    "\n",
    "# X_train_scaled = pd.read_csv('X_train_augmented1.csv')\n",
    "# y_train = pd.read_csv('y_train_augmented1.csv')['Label']\n",
    "# X_test_scaled= pd.read_csv('X_test_original.csv')\n",
    "# y_test = pd.read_csv('y_test_original.csv')['Label']\n",
    "\n",
    "\n",
    "# # # Standardize features\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# # X_train_scaled = scaler.fit_transform(X_train)\n",
    "# # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize models and parameter grids\n",
    "# models = {\n",
    "#     # \"SVM\": {\n",
    "#     #     \"model\": SVC(),\n",
    "#     #     \"params\": {\n",
    "#     #         'C':      [0.1, 0.5, 1, 5, 10, 100],\n",
    "#     #         'gamma':  ['scale', 'auto'],\n",
    "#     #         'kernel': ['poly', 'rbf', 'sigmoid','linear'],\n",
    "#     #         'random_state': [10]\n",
    "            \n",
    "#     #         }\n",
    "#     # },\n",
    "#     # \"Decision Tree\": {\n",
    "#     #     \"model\": DecisionTreeClassifier(),\n",
    "#     #     \"params\": {\n",
    "#     #         'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "#     #         'min_samples_split': np.arange(2, 9),\n",
    "#     #         'min_samples_leaf': [1, 2, 3, 4],\n",
    "#             'criterion': ['gini', 'entropy'],\n",
    "#     #         'splitter': ['best', 'random'],\n",
    "#     #         'random_state': [10]\n",
    "\n",
    "#     #     }\n",
    "#     # },\n",
    "#     \"Random Forest\": {\n",
    "#         \"model\": RandomForestClassifier(),\n",
    "#         \"params\": {\n",
    "#             'n_estimators': [50,100, 200, 300],\n",
    "#             'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "#             'min_samples_split': np.arange(2, 9),\n",
    "#             'criterion': ['gini', 'entropy'],\n",
    "#             'max_features': ['sqrt', 'log2', 1.0],\n",
    "#             'random_state': [10]\n",
    "            \n",
    "\n",
    "#         }\n",
    "#     },\n",
    "#     \"Gradient Boost\": {\n",
    "#         \"model\": GradientBoostingClassifier(),\n",
    "#         \"params\": {\n",
    "#             'n_estimators': [50, 100, 200, 300],\n",
    "#             'learning_rate': [0.01, 0.1],\n",
    "#             'max_depth':[None, 2, 3, 5, 7, 8, 10],\n",
    "#             'min_samples_leaf': [1, 2, 3, 4],\n",
    "#             'subsample': [0.8, 1.0],\n",
    "#             'min_samples_split': np.arange(2, 9),\n",
    "#             'random_state': [10]  \n",
    "\n",
    "#         }\n",
    "#     },\n",
    "#     \"XGBoost\": {\n",
    "#         \"model\": XGBClassifier(),\n",
    "#         \"params\": {\n",
    "#             'n_estimators': [50, 100, 200, 300],\n",
    "#             'learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "#             'max_depth': [None, 2, 3, 5, 7, 8, 10],\n",
    "#             'subsample':      [0.5, 1.0],\n",
    "#             'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "#             'gamma': [0, 0.1, 0.2],\n",
    "#             # 'scale_pos_weight': [1, 2, 5],  # Handles class imbalance\n",
    "            \n",
    "#             # 'eval_metric': ['mlogloss'],\n",
    "#             # 'min_samples_split': np.arange(2, 9),\n",
    "#             'random_state': [10]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Train/evaluate models\n",
    "# results = {}\n",
    "# for name, config in models.items():\n",
    "#     print(f\"\\n======= Training {name} =======\")\n",
    "    \n",
    "#     # Randomized search with 5-fold stratified CV\n",
    "#     search = RandomizedSearchCV(\n",
    "#         estimator=config[\"model\"],\n",
    "#         param_distributions=config[\"params\"],\n",
    "#         n_iter=10,\n",
    "#         cv=StratifiedKFold(n_splits=5 , random_state=42, shuffle=True),\n",
    "#         scoring='f1_weighted',\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Best model evaluation\n",
    "#     best_model = search.best_estimator_\n",
    "#     y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "#     # Store results\n",
    "#     results[name] = {\n",
    "#         \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "#         \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "#         \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "#         \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "#         \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "#         # \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "#         \"balanced_accuracy\": (sensitivity_score(y_test, y_pred , average='weighted') + specificity_score(y_test, y_pred, average='weighted')) / 2,\n",
    "\n",
    "#         \"best_params\": search.best_params_\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Best Parameters: {search.best_params_}\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Compare model performances\n",
    "# print(\"\\n===== Model Performance Summary =====\")\n",
    "# for model, metrics in results.items():\n",
    "#     print(f\"\\n{model}:\")\n",
    "#     print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "#     print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "#     print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "#     print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "#     print(f\"Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "#     # print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "#     print(f\"balanced_accuracy\", metrics['balanced_accuracy'])\n",
    "#     print(f\"Best Parameters: {metrics['best_params']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
